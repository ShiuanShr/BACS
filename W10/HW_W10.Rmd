---
title: "HW10"
author: '110078509  & I discussed with 109065707'
date: '20220423'
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r setup, include=FALSE} 
#remove the random variable to fresh the working environment 
#rm(list=ls()) 
knitr::opts_chunk$set(echo = TRUE) 
library(tidyverse) 
library(ggplot2) 
library(dplyr)
library(corrplot)
#library(coefplot)
library(car)

norm_qq_plot <- function(values) {
 probs1000 <- seq(0, 1, 0.001)
 q_vals <- quantile(values, probs = probs1000)
 q_norm <- qnorm(probs1000 ,mean = mean(values), sd = sd(values))
 plot(q_norm, q_vals, xlab="normal quantiles", ylab="values quantiles")
 abline(a=0,b=1,col=2,lwd=3)
}
```


#### Question 1

a.	Comparing scenarios 1 and 2, which do we expect to have a stronger R2 ?

- Ans: Scenarios 1

b.	Comparing scenarios 3 and 4, which do we expect to have a stronger R2 ?

- Ans: Scenarios 3

c.	Comparing scenarios 1 and 2, which do we expect has bigger/smaller SSE, SSR, and SST? (intuitively)

- Ans: Intuitively, scenarios 2 have bigger SSR and SST. For the part of SSR, I think scenarios 1 having slighly bigger one.

d.	Comparing scenarios 3 and 4, which do we expect has bigger/smaller SSE, SSR, and SST? (intuitively)

- Ans: Intuitively,scenarios 4 has a bigger SST, SSE. For the part of SSR, it's not obvious to tell.


-------------------------------------------------------------------



#### Question 2

a. 
```{r}
prog <- read.csv("programmer_salaries.txt", sep="\t", header=TRUE)
head(prog,3)
```

- Coefficients:
```{r}
# Raw data regression
prog_regr <- lm( Salary ~ Experience + Score + Degree , data =prog ) 
summary(prog_regr)

```
-R Square:   Multiple R-squared:  0.8468,	Adjusted R-squared:  0.8181 
 
 (1). First 5 values
```{r}
#  (1). First 5 values of y HAT
head(prog_regr$fitted.values,5)

```

```{r}
# (2). epsilon ε
head(prog_regr$residuals,5)
```

-------------------------------------------------------------------


#### b. Linear algebra 


#### i.  Create an X matrix that has a first column of 1s followed by columns of the independent variables
(only show the code)


```{r}
X <- cbind(Intercept =1,as.matrix(prog[1:3]))
```

-------------------------------------------------------------------

#### ii.    Create a y vector with the Salary values 
(only show the code)

```{r}
mat.data4 <- prog$Salary
y <- matrix(mat.data4,ncol = 1)
```

-------------------------------------------------------------------

#### iii.   Compute the beta_hat vector of estimated regression coefficients (show the code and values)

```{r}
beta_hat <- (solve(t(X)%*%X)) %*% t(X) %*%y
beta_hat
```

------------------------

#### iv. Compute a y_hat vector of estimated y hat values, and a res vector of residuals 
(show the code and the first 5 values of y_hat and res)

- y hat
```{r}
y_hat <- X %*% beta_hat
head(y_hat,5)
```
- res vector of residuals 

```{r}
residual <- y -y_hat; 
head(residual,5)
```

---

#### v.	Using only the results from (i) – (iv), compute SSR, SSE and SST (show the code and values)
```{r}
SSE <- sum((y -y_hat)^2)
SSR <- sum(( y_hat - mean(y))^2)
SST <- SSE + SSR

sprintf('SSE: %f ; SSR: %f; SST: %f', SSE,SSR, SST)
```

---

#### c. Compute R2 for in two ways, and confirm you get the same results (show code and values):

#### i. 	Use any combination of SSR, SSE, and SST

```{r}
r.square = SSR/SST ; 
r.square
```

####  ii. Use the squared correlation of vectors y and y hat

```{r}
cor(y, y_hat) ^2
```
- Ans: Yes. They're the same.

### Question 3 


- Data preprocessing
```{r}
auto <- read.table("auto-data.txt", header=FALSE, na.strings = "?")
names(auto) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", 
                 "acceleration", "model_year", "origin", "car_name")
auto$origin <-as.factor(auto$origin)
auto<-na.omit(auto)
head(auto,10)
```

See whether there is any strong cor first

- Exploratory analysis:

```{r}
plot(auto)
```

Except origin & car_name, the rest of column are numeric.
Hence I plot it as corrplot to have a basic understanding of the dataset.

```{r}
options(repr.plot.width = 14, repr.plot.height = 8) 
cor_features <- cor(auto[,1:7], method='spearman')
corrplot::corrplot(cor_features, tl.cex=0.6, type='lower', method="ellipse")
```

```{r}
summary(auto)
```

#### a.	Let’s first try exploring this data and problem:

#### i.	Visualize the data in any way you feel relevant (report only relevant/interesting ones)


```{r}
ggplot(auto, aes(x = mpg, y = weight)) + geom_point(aes(color = cylinders))
```

- *Ans: *  This one is pretty interesting, isn't it? 

The insight is :

In general, the mpg has a negative relation with the weight of car, which is quite intuitively.
However, after grouping, even when the car weight are close, we can tell that there is a huge variance among the mpg performance when the cylinder numbers is limited (The dark blue data points).

Comparing to the former , the mpg performance of the multi-cylinder car is not that disperse as their car weight are alike (the light blue data points). 

My friend told me the reason behind it is because as numbers of cylinder decreases, the impact  of the 'cylinder design' increases based on the functional positioning.



Notice that: cylinder is  multi-valued discrete data type, which is used for grouping as above.

---

####  ii.	Report a correlation table of all variables, rounding to two decimal places (in the cor() function, set use="pairwise.complete.obs" to handle missing values)


- *Ans: *  Examine the correlations between variables and then examine relationships 1 by 1.

- ps: We skipped the last 2 columns because they are not continuous variable.

```{r}
round(cor(auto[,c(1:7)], use="pairwise.complete.obs"),2)

```

----


#### iii. From the visualizations and correlations, which variables seem to relate to mpg?

- *Ans: * 

cylinders , displacement , horsepower, weight are negative to mpg. Other features are positive relate to mpg.       

---

#### iv.	Which relationships might not be linear? 


- *Ans: * 

Any relation between the rest of the column to the colname of rigin & car_name will not be linear.

Because they are not continuous variable. Therefore, I did not show them in the table above as it's redundant information. 

In the table above, as the the data points format into symmetrical but non-linear shape, the correlation will getting close to 0. Therefore, I guessed the correlation(0.29) between model_year and acceleration will be non-linear.

---

#### v. Are there any pairs of independent variables that are highly correlated (r > 0.7)?


- *Ans: * Yes, for example, cylinders & displacement (0.95), weight  & displacement (0.93) etc.


---

#### b.	Let’s create a linear regression model where mpg is dependent upon all other suitable variables 

```{r}

summary(lm(mpg ~ cylinders+displacement+horsepower+weight+acceleration+model_year+origin, data = auto))

```

#### i.Which independent variables have a ‘significant’ relationship with mpg at 1% significance?

- Ans:

displacement  , weight , model_year, origin2 , origin3 


---


#### ii.	Looking at the coefficients, is it possible to determine which independent variables are the most effective at increasing mpg? If so, which ones, and if not, why not? (hint: units!)


First thing first, we select the positive correlation one. And see the R Square individually.

```{r}
summary(lm(mpg~acceleration , data = auto))[8]

summary(lm(mpg~model_year, data = auto))[8]

```
*Explain:*

For these 2 positive correlation to mpg columns (acceleration,model_year), the r square of model_year is higher than acceleration. Which indicates that SSE/SST is smaller in the individual ratio. If the sum square of error(residual error sum) is smaller, means the predictive linear line fit the y data points(mpg) much better as the x-axis is model_year.



-----------------------------------------------


#### c.Let’s try to resolve some of the issues with our regression model above.

#### i.	Create fully standardized regression results: are these slopes easier to compare?
(note: consider if you should standardize origin) 

- *Ans. *:Sure! The slopes of standard regression are easier to compare because they are in the same scale. Additionaly,  we should not standardize origin cuz it's categorical variable.


```{r}

# [1:7] mpg to model_year (without origin and car_names)

auto_std<-data.frame(scale(auto[,1:7]))

auto_std<-cbind(auto_std, origin = auto$origin)

lm_std <- lm(mpg ~ cylinders+displacement+horsepower+weight+acceleration+model_year+origin, data = auto_std)
summary(lm_std)
```
##### ii. Regress mpg over each non-significant independent variable, individually.
Which ones become significant when we regress mpg over them individually?

- *Ans. *: First, we pick the non-significant features :  cylinders , horsepower and  acceleration.


```{r}
summary(lm(mpg~ cylinders , data = auto))[4]

summary(lm(mpg~ horsepower, data = auto))[4]
summary(lm(mpg ~ acceleration, data = auto))[4]

```
Due to the results above, all three (the  non-significant features) become significant after standardization.


##### iii.	Plot the density of the residuals: are they normally distributed and centered around zero? (get the residuals of a fitted linear model, e.g. regr <- lm(...), using regr$residuals


- *Ans. *:


About the question of whether it's normally distributed (centered at 0), we supposed to do the further QQ plot & shapiro.test as below: 

```{r}
plot(density(lm_std$residuals), main='Density of the residuals of the standardized auto dataset lm model', lwd=2) + abline(v= mean(lm_std$residuals), col = "red", lwd = 2)
```

For the plot of QQ- plot, we can tell that sth strange in the right tail of the data points.

```{r}
norm_qq_plot(lm_std$residuals)
```

Hence, we do the further shapiro.test.
As the p-value below, the p-value < 0.01, we can reject the default H0. 
Which means that residuals is not following normal distribution. 

```{r}
shapiro.test(lm_std$residuals)
```
